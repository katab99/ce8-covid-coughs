{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Miniproject - Audio\n",
    "\n",
    "AVS 8th Semester - Group 841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from numba import jit\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim \n",
    "#from transformers import ASTFeatureExtractor\n",
    "#from torch.utils.data import random_split, DataLoader, Dataset\n",
    "#import pytorch_lightning as pl\n",
    "import torchaudio as ta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Explore the dataset through code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many samples does the dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples : 27550\n"
     ]
    }
   ],
   "source": [
    "#Load the data!\n",
    "PATH = \"./data/metadata_compiled.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "#Check no. samples\n",
    "print(f'Number of samples : {df.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. How many classes? How many samples per class? Show a histogram of the number of intances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of classes: {len(df[\"status\"].unique())}.\\n\\\n",
    "    Classes: {df[\"status\"].unique()}\\n\\\n",
    "    {pd.value_counts(df[\"status\"], dropna=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some problem with this shit\n",
    "pd.value_counts(df['status'], dropna=False).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Play a random sample from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing healthy\n",
    "healthy = df[df['status'] == 'healthy'].sample()['uuid'].item()\n",
    "path = \"./conv/\" + healthy + '.mp3'\n",
    "y, sr = librosa.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing COVID-19\n",
    "covid = df[df['status'] == 'COVID-19'].sample()['uuid'].item()\n",
    "path = \"./conv/\" + covid + '.mp3'\n",
    "y, sr = librosa.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing symptomatic\n",
    "symptomatic = df[df['status'] == 'symptomatic'].sample()['uuid'].item()\n",
    "path = \"./conv/\" + symptomatic + '.mp3'\n",
    "y, sr = librosa.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Describe if/how you think the data distribution will affect training of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Decide what part of the dataset to use; all, some classes, some samples. Motivate your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use a neural network of your own chose to classify the dataset. Explain your choice and at least one alternative. Document your experiences:."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Spectogram Transformer Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Dataset class is created to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatatset(Dataset):\n",
    "    def __init__(self, audio_dir, class_csv):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.class_csv = class_csv\n",
    "\n",
    "        self.audio_dir_list = os.listdir(self.audio_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_dir_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_file_path = os.path.join(self.audio_dir, self.audio_dir_list[idx])\n",
    "        #print(\"lol: l, self.audio_dir_list[idx]: \", self.audio_dir_list[idx])\n",
    "        \n",
    "        waveform, sample_rate = ta.load(audio_file_path, normalize=True)\n",
    "        \n",
    "        #print(type(waveform))\n",
    "        #print(sample_rate)\n",
    "        transform = ta.transforms.MelSpectrogram(sample_rate, n_mels=32)\n",
    "        mel_specgram = transform(waveform) \n",
    "\n",
    "        audio_file_name = self.audio_dir_list[idx].replace('.mp3', '')\n",
    "\n",
    "        #Transform \n",
    "        #Spectogram\n",
    "\n",
    "        i = df[df['uuid']==audio_file_name].index.values\n",
    "        label = df[\"status\"].loc[df.index[i].values[0]]\n",
    "\n",
    "        return mel_specgram, label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = AudioDatatset(\"./data\", \"metadata_compiled.csv\")\n",
    "print(test.__getitem__(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModuleClass(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=12):\n",
    "        super().__init__()\n",
    "        #self.transform = transforms.MelSpectrogram(sample_rate)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        pass\n",
    "        #Define steps that should be done\n",
    "        #only on one GPU, like getting data\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        #Apply melSpectogram transform\n",
    "        self.audio_files =  AudioDatatset(\"./data\", \"metadata_compiled.csv\")\n",
    "        self.train_data, self.valid_data, self.test_data = random_split(self.audio_files, [0.7,0.2,0.1], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        #Define steps that should be done on every GPU,\n",
    "        #like splitting data, applying transform etc.\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, self.batch_size)        \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_data, self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, self.batch_size)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "data_set = DataModuleClass(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.setup()\n",
    "data_train = data_set.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, samples in enumerate(data_train):\n",
    "    print(batch_idx, samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading and pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 16226/16226 [00:00<00:00, 20650.88it/s] \n",
      "Found cached dataset audiofolder (C:/Users/Kata/.cache/huggingface/datasets/audiofolder/default-800512f330983e37/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# must have metadata.csv with 'file_name' column to have also the features\n",
    "# unsplitted dataset\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./data\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting up the data : {training - 70%, validation - 20%, test - 10%}\n",
    "# shuffle=True\n",
    "train_testvalid = dataset.train_test_split(test_size=0.3)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train' : train_testvalid['train'],\n",
    "    'test' : test_valid['test'],\n",
    "    'valid' : test_valid['train']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the labels for classification\n",
    "labels = list(df[\"status\"].unique()[1:])\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 1\n",
    "\n",
    "samplin_rate = 16000\n",
    "batch_size = 8\n",
    "num_classes = len(labels)\n",
    "learning_rate = 1e-4\n",
    "hidden_dim = 768\n",
    "\n",
    "max_seq_length = max_duration * samplin_rate\n",
    "max_frames = 49\n",
    "max_epochs = 2\n",
    "\n",
    "\n",
    "is_cuda = False # torch.cuda.is_available()\n",
    "checkpoint = 'MIT/ast-finetuned-audioset-10-10-0.4593'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification\n",
    "# ASTConfig, \n",
    "\n",
    "# must be the same -> model and tokenizer/feature extractor\n",
    "# right now with default values\n",
    "\n",
    "#config = ASTConfig()\n",
    "\n",
    "# basically tokenizer\n",
    "# input normalization: mean = 0, std = 0.5\n",
    "\n",
    "#feature_extractor = ASTFeatureExtractor(config, sampling_rate=sr, num_mel_bins=32, mean=0, std=0.5)\n",
    "\n",
    "#model = ASTForAudioClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ASTFeatureExtractor(\n",
    "    checkpoint,\n",
    "    return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature extraction - using map()\n",
    "def preprocess(examples):\n",
    "    audio_arr = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arr,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m      2\u001b[0m     preprocess, remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m], batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49mbatch_size\n\u001b[0;32m      3\u001b[0m     )\n\u001b[0;32m      5\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(\n\u001b[0;32m      6\u001b[0m     preprocess, remove_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m], batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39mbatch_size\n\u001b[0;32m      7\u001b[0m     )\n\u001b[0;32m      9\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(\n\u001b[0;32m     10\u001b[0m     preprocess, remove_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m], batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39mbatch_size\n\u001b[0;32m     11\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    562\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 563\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    564\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    565\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    566\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    522\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    526\u001b[0m }\n\u001b[0;32m    527\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 528\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    529\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    530\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_dataset.py:2953\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2945\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2946\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   2947\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   2948\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2951\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2952\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 2953\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   2954\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   2955\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_dataset.py:3346\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3344\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(batch)\n\u001b[0;32m   3345\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3346\u001b[0m         writer\u001b[39m.\u001b[39;49mwrite_batch(batch)\n\u001b[0;32m   3347\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3348\u001b[0m \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m _time \u001b[39m+\u001b[39m config\u001b[39m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_writer.py:555\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    553\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n\u001b[0;32m    554\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n\u001b[1;32m--> 555\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_writer.py:573\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[1;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_bytes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mnbytes\n\u001b[0;32m    572\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_examples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mnum_rows\n\u001b[1;32m--> 573\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpa_writer\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\pyarrow\\ipc.pxi:525\u001b[0m, in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\fsspec\\implementations\\local.py:352\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 352\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf\u001b[39m.\u001b[39mwrite(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].map(\n",
    "    preprocess, remove_columns=[\"audio\"], batched=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "dataset['valid'] = dataset['valid'].map(\n",
    "    preprocess, remove_columns=[\"audio\"], batched=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "dataset['test'] = dataset['test'].map(\n",
    "    preprocess, remove_columns=[\"audio\"], batched=True, batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=len(labels),\n",
    "    ignore_mismatched_sizes=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001A61E82C040>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = \"adamw_torch\"\n",
    "#torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# the hyperparams for Trainer\n",
    "training_arg = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=optimizer,\n",
    "    num_train_epochs= max_epochs,\n",
    "    save_strategy=\"epoch\",\n",
    "    no_cuda=is_cuda\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_arg,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['valid'],\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "predictions = trainer.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model weigths into files\n",
    "model.save_pretrained('./saved_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=epochs,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    save_total_limit=1, # limit the total amount of checkpoints, deletes older chps in output_dir\n",
    "    per_device_train_batch_size = batchsize,\n",
    "    per_device_eval_batch_size = batchsize,\n",
    "    logging_steps=500,\n",
    ")\n",
    "#report_to=\"tensorboard\",\n",
    "\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_cosine(100,max_lr=lr,total_steps=total_steps, optimizer_lr=lr,min_lr=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset= None, # TODO\n",
    "    eval_dataset=None, # TODO\n",
    "    #tokenizer=feature_extractor, TODO\n",
    "    compute_metrics=compute_metrics,\n",
    "    #optimizers=(optimizer, scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Discuss at least four relevant hyper-parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate : \\\n",
    "Epoch Numer : \\\n",
    "Batch Size : \\\n",
    "Optimizer: \\\n",
    "Layer Number : [??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "# epoch number\n",
    "# mini-batch size\n",
    "\n",
    "# I don't know what is the 4th one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Experiment with the effect of different batch sizes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Experiment with the effect of different learning rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Experiment with different number of network layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Implement at least two data agumentation techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Discuss what influences the memory use of a solution such as yours. What can be done to reduce this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "176a6de24af4388d6588f5f4b62d8682e845efb0db5cda775d003c010c4ffac3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
