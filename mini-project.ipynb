{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Miniproject - Audio\n",
    "\n",
    "AVS 8th Semester - Group 841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio as ta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from numba import jit\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim \n",
    "#from transformers import ASTFeatureExtractor\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparaeters\n",
    "\n",
    "AUDIO_DIR = \"./data/Data/\"\n",
    "CSV_DIR = \"./data/metadata_compiled.csv\"\n",
    "FILE_TYPE = \".wav\"\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Explore the dataset through code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many samples does the dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples : 27550\n"
     ]
    }
   ],
   "source": [
    "#Load the data!\n",
    "df = pd.read_csv(CSV_DIR)\n",
    "\n",
    "#Check no. samples\n",
    "print(f'Number of samples : {df.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. How many classes? How many samples per class? Show a histogram of the number of intances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4.\n",
      "    Classes: [nan 'healthy' 'COVID-19' 'symptomatic']\n",
      "    healthy        12479\n",
      "NaN            11326\n",
      "symptomatic     2590\n",
      "COVID-19        1155\n",
      "Name: status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of classes: {len(df[\"status\"].unique())}.\\n\\\n",
    "    Classes: {df[\"status\"].unique()}\\n\\\n",
    "    {pd.value_counts(df[\"status\"], dropna=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some problem with this shit\n",
    "pd.value_counts(df['status'], dropna=False).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Play a random sample from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_343181/1290488024.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/Data/3fc6450d-d5a3-47a0-8f31-60139f6fe272.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     y, sr_native \u001b[39m=\u001b[39m __soundfile_load(path, offset, duration, dtype)\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m sf\u001b[39m.\u001b[39mSoundFileRuntimeError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m     \u001b[39m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/librosa/core/audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     context \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39;49mSoundFile(path)\n\u001b[1;32m    211\u001b[0m \u001b[39mwith\u001b[39;00m context \u001b[39mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info \u001b[39m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[39mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(file, mode_int, closefd)\n\u001b[1;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(mode)\u001b[39m.\u001b[39missuperset(\u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[39m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     err \u001b[39m=\u001b[39m _snd\u001b[39m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[39mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError opening \u001b[39m\u001b[39m{0!r}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[39mif\u001b[39;00m mode_int \u001b[39m==\u001b[39m _snd\u001b[39m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[39m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[39m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[39m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening './data/Data/3fc6450d-d5a3-47a0-8f31-60139f6fe272.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m healthy \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhealthy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msample()[\u001b[39m'\u001b[39m\u001b[39muuid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m      3\u001b[0m path \u001b[39m=\u001b[39m AUDIO_DIR \u001b[39m+\u001b[39m healthy \u001b[39m+\u001b[39m FILE_TYPE\n\u001b[0;32m----> 4\u001b[0m y, sr \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mload(path)\n\u001b[1;32m      5\u001b[0m ipd\u001b[39m.\u001b[39mAudio(y, rate\u001b[39m=\u001b[39msr)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/librosa/core/audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, (\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPurePath)):\n\u001b[1;32m    181\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[39m\"\u001b[39m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     y, sr_native \u001b[39m=\u001b[39m __audioread_load(path, offset, duration, dtype)\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/librosa/util/decorators.py:60\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     53\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mDeprecated as of librosa version \u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mIt will be removed in librosa version \u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,  \u001b[39m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/librosa/core/audio.py:241\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    238\u001b[0m     reader \u001b[39m=\u001b[39m path\n\u001b[1;32m    239\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[39m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     reader \u001b[39m=\u001b[39m audioread\u001b[39m.\u001b[39;49maudio_open(path)\n\u001b[1;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m reader \u001b[39mas\u001b[39;00m input_file:\n\u001b[1;32m    244\u001b[0m     sr_native \u001b[39m=\u001b[39m input_file\u001b[39m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/audioread/__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m BackendClass \u001b[39min\u001b[39;00m backends:\n\u001b[1;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m BackendClass(path)\n\u001b[1;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    129\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fh \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     61\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m aifc\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/Data/3fc6450d-d5a3-47a0-8f31-60139f6fe272.wav'"
     ]
    }
   ],
   "source": [
    "# playing healthy\n",
    "healthy = df[df['status'] == 'healthy'].sample()['uuid'].item()\n",
    "path = AUDIO_DIR + healthy + FILE_TYPE\n",
    "y, sr = librosa.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_343181/34731780.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/Data/dbb52561-93ca-444e-9ff9-e61fd6c6996f.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     y, sr_native \u001b[39m=\u001b[39m __soundfile_load(path, offset, duration, dtype)\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m sf\u001b[39m.\u001b[39mSoundFileRuntimeError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m     \u001b[39m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/librosa/core/audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     context \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39;49mSoundFile(path)\n\u001b[1;32m    211\u001b[0m \u001b[39mwith\u001b[39;00m context \u001b[39mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info \u001b[39m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[39mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(file, mode_int, closefd)\n\u001b[1;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(mode)\u001b[39m.\u001b[39missuperset(\u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[39m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     err \u001b[39m=\u001b[39m _snd\u001b[39m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[39mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError opening \u001b[39m\u001b[39m{0!r}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[39mif\u001b[39;00m mode_int \u001b[39m==\u001b[39m _snd\u001b[39m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[39m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[39m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[39m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening './data/Data/dbb52561-93ca-444e-9ff9-e61fd6c6996f.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m covid \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mCOVID-19\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msample()[\u001b[39m'\u001b[39m\u001b[39muuid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m      3\u001b[0m path \u001b[39m=\u001b[39m AUDIO_DIR \u001b[39m+\u001b[39m covid \u001b[39m+\u001b[39m FILE_TYPE\n\u001b[0;32m----> 4\u001b[0m y, sr \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mload(path)\n\u001b[1;32m      5\u001b[0m ipd\u001b[39m.\u001b[39mAudio(y, rate\u001b[39m=\u001b[39msr)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/librosa/core/audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, (\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPurePath)):\n\u001b[1;32m    181\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[39m\"\u001b[39m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     y, sr_native \u001b[39m=\u001b[39m __audioread_load(path, offset, duration, dtype)\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/librosa/util/decorators.py:60\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     53\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mDeprecated as of librosa version \u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mIt will be removed in librosa version \u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,  \u001b[39m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/librosa/core/audio.py:241\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    238\u001b[0m     reader \u001b[39m=\u001b[39m path\n\u001b[1;32m    239\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[39m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     reader \u001b[39m=\u001b[39m audioread\u001b[39m.\u001b[39;49maudio_open(path)\n\u001b[1;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m reader \u001b[39mas\u001b[39;00m input_file:\n\u001b[1;32m    244\u001b[0m     sr_native \u001b[39m=\u001b[39m input_file\u001b[39m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/audioread/__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m BackendClass \u001b[39min\u001b[39;00m backends:\n\u001b[1;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m BackendClass(path)\n\u001b[1;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    129\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.10/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fh \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     61\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m aifc\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/Data/dbb52561-93ca-444e-9ff9-e61fd6c6996f.wav'"
     ]
    }
   ],
   "source": [
    "# playing COVID-19\n",
    "covid = df[df['status'] == 'COVID-19'].sample()['uuid'].item()\n",
    "path = AUDIO_DIR + covid + FILE_TYPE\n",
    "y, sr = librosa.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing symptomatic\n",
    "symptomatic = df[df['status'] == 'symptomatic'].sample()['uuid'].item()\n",
    "path =  AUDIO_DIR + symptomatic + FILE_TYPE\n",
    "y, sr = librosa.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Describe if/how you think the data distribution will affect training of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Decide what part of the dataset to use; all, some classes, some samples. Motivate your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use a neural network of your own chose to classify the dataset. Explain your choice and at least one alternative. Document your experiences:."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Spectogram Transformer Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Dataset class is created to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatatset(Dataset):\n",
    "    def __init__(self, audio_dir, class_csv):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.df = pd.read_csv(class_csv)\n",
    "\n",
    "        self.audio_dir_list = os.listdir(self.audio_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_dir_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_file_path = os.path.join(self.audio_dir, self.audio_dir_list[idx])\n",
    " \n",
    "        waveform, sample_rate = ta.load(audio_file_path, normalize=True)\n",
    "        \n",
    "        #print(type(waveform))\n",
    "        #print(sample_rate)\n",
    "        transform = ta.transforms.MelSpectrogram(sample_rate, n_mels=32)\n",
    "        mel_specgram = transform(waveform) \n",
    "\n",
    "        audio_file_name = self.audio_dir_list[idx].replace(FILE_TYPE, '')\n",
    "\n",
    "        #Transform \n",
    "        #Spectogram\n",
    "\n",
    "        i =  self.df[ self.df['uuid']==audio_file_name].index.values\n",
    "        label =  self.df[\"status\"].loc[ self.df.index[i].values[0]]\n",
    "\n",
    "        return mel_specgram, label \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.6694e-06,\n",
      "          5.7855e-06, 3.0521e-06],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2238e-06,\n",
      "          4.7898e-06, 2.5738e-06],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0420e-06,\n",
      "          6.5483e-08, 1.6850e-07],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6220e-07,\n",
      "          3.0945e-08, 7.8637e-08],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1619e-07,\n",
      "          2.4514e-08, 1.1150e-07],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.9584e-08,\n",
      "          1.8637e-08, 1.2825e-07]]]), 'healthy')\n",
      "2214\n"
     ]
    }
   ],
   "source": [
    "test = AudioDatatset(AUDIO_DIR, CSV_DIR)\n",
    "print(test.__getitem__(0))\n",
    "print(test.__len__())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModuleClass(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        #self.transform = transforms.MelSpectrogram(sample_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.audio_files =  []\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        pass\n",
    "        #Define steps that should be done\n",
    "        #only on one GPU, like getting data\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        #Apply melSpectogram transform\n",
    "        self.audio_files = AudioDatatset(AUDIO_DIR, CSV_DIR)\n",
    "\n",
    "        #Splitting manually \n",
    "        audio_len = self.audio_files.__len__()\n",
    "        train_size = round(audio_len * 0.7)\n",
    "        val_size = round(audio_len * 0.2)\n",
    "        test_size = audio_len - train_size - val_size\n",
    "        \n",
    "        self.train_data, self.val_data, self.test_data = random_split(self.audio_files, [train_size, val_size, test_size])\n",
    "\n",
    "        #Define steps that should be done on every GPU,\n",
    "        #like splitting data, applying transform etc.\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, self.batch_size)        \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, self.batch_size)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = DataModuleClass(BATCH_SIZE)\n",
    "data_set.setup()\n",
    "data_train = data_set.train_dataloader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading and pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 16226/16226 [00:00<00:00, 20650.88it/s] \n",
      "Found cached dataset audiofolder (C:/Users/Kata/.cache/huggingface/datasets/audiofolder/default-800512f330983e37/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# must have metadata.csv with 'file_name' column to have also the features\n",
    "# unsplitted dataset\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"./data\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting up the data : {training - 70%, validation - 20%, test - 10%}\n",
    "# shuffle=True\n",
    "train_testvalid = dataset.train_test_split(test_size=0.3)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train' : train_testvalid['train'],\n",
    "    'test' : test_valid['test'],\n",
    "    'valid' : test_valid['train']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the labels for classification\n",
    "labels = list(df[\"status\"].unique()[1:])\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 1\n",
    "\n",
    "samplin_rate = 16000\n",
    "batch_size = 8\n",
    "num_classes = len(labels)\n",
    "learning_rate = 1e-4\n",
    "hidden_dim = 768\n",
    "\n",
    "max_seq_length = max_duration * samplin_rate\n",
    "max_frames = 49\n",
    "max_epochs = 2\n",
    "\n",
    "\n",
    "is_cuda = False # torch.cuda.is_available()\n",
    "checkpoint = 'MIT/ast-finetuned-audioset-10-10-0.4593'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification\n",
    "# ASTConfig, \n",
    "\n",
    "# must be the same -> model and tokenizer/feature extractor\n",
    "# right now with default values\n",
    "\n",
    "#config = ASTConfig()\n",
    "\n",
    "# basically tokenizer\n",
    "# input normalization: mean = 0, std = 0.5\n",
    "\n",
    "#feature_extractor = ASTFeatureExtractor(config, sampling_rate=sr, num_mel_bins=32, mean=0, std=0.5)\n",
    "\n",
    "#model = ASTForAudioClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ASTFeatureExtractor(\n",
    "    checkpoint,\n",
    "    return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature extraction - using map()\n",
    "def preprocess(examples):\n",
    "    audio_arr = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arr,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m      2\u001b[0m     preprocess, remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m], batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49mbatch_size\n\u001b[0;32m      3\u001b[0m     )\n\u001b[0;32m      5\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(\n\u001b[0;32m      6\u001b[0m     preprocess, remove_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m], batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39mbatch_size\n\u001b[0;32m      7\u001b[0m     )\n\u001b[0;32m      9\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(\n\u001b[0;32m     10\u001b[0m     preprocess, remove_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m], batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39mbatch_size\n\u001b[0;32m     11\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    562\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 563\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    564\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    565\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    566\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    522\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    526\u001b[0m }\n\u001b[0;32m    527\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 528\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    529\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    530\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_dataset.py:2953\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2945\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2946\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   2947\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   2948\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2951\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2952\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 2953\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   2954\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   2955\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_dataset.py:3346\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3344\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(batch)\n\u001b[0;32m   3345\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3346\u001b[0m         writer\u001b[39m.\u001b[39;49mwrite_batch(batch)\n\u001b[0;32m   3347\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3348\u001b[0m \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m _time \u001b[39m+\u001b[39m config\u001b[39m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_writer.py:555\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    553\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n\u001b[0;32m    554\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n\u001b[1;32m--> 555\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\datasets\\arrow_writer.py:573\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[1;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_bytes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mnbytes\n\u001b[0;32m    572\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_examples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mnum_rows\n\u001b[1;32m--> 573\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpa_writer\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\pyarrow\\ipc.pxi:525\u001b[0m, in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Kata\\anaconda3\\envs\\deepl\\lib\\site-packages\\fsspec\\implementations\\local.py:352\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 352\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf\u001b[39m.\u001b[39mwrite(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].map(\n",
    "    preprocess, remove_columns=[\"audio\"], batched=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "dataset['valid'] = dataset['valid'].map(\n",
    "    preprocess, remove_columns=[\"audio\"], batched=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "dataset['test'] = dataset['test'].map(\n",
    "    preprocess, remove_columns=[\"audio\"], batched=True, batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=len(labels),\n",
    "    ignore_mismatched_sizes=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001A61E82C040>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = \"adamw_torch\"\n",
    "#torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# the hyperparams for Trainer\n",
    "training_arg = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=optimizer,\n",
    "    num_train_epochs= max_epochs,\n",
    "    save_strategy=\"epoch\",\n",
    "    no_cuda=is_cuda\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_arg,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['valid'],\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "predictions = trainer.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model weigths into files\n",
    "model.save_pretrained('./saved_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=epochs,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    save_total_limit=1, # limit the total amount of checkpoints, deletes older chps in output_dir\n",
    "    per_device_train_batch_size = batchsize,\n",
    "    per_device_eval_batch_size = batchsize,\n",
    "    logging_steps=500,\n",
    ")\n",
    "#report_to=\"tensorboard\",\n",
    "\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_cosine(100,max_lr=lr,total_steps=total_steps, optimizer_lr=lr,min_lr=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset= None, # TODO\n",
    "    eval_dataset=None, # TODO\n",
    "    #tokenizer=feature_extractor, TODO\n",
    "    compute_metrics=compute_metrics,\n",
    "    #optimizers=(optimizer, scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Discuss at least four relevant hyper-parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate : \\\n",
    "Epoch Numer : \\\n",
    "Batch Size : \\\n",
    "Optimizer: \\\n",
    "Layer Number : [??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "# epoch number\n",
    "# mini-batch size\n",
    "\n",
    "# I don't know what is the 4th one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Experiment with the effect of different batch sizes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Experiment with the effect of different learning rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Experiment with different number of network layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Implement at least two data agumentation techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Discuss what influences the memory use of a solution such as yours. What can be done to reduce this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3f78a94c79aa2b4cde3db52c6d7f9e98cfa173f2ab20b939f1d7db0be852c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
