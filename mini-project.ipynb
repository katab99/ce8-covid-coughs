{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Miniproject - Audio\n",
    "\n",
    "AVS 8th Semester - Group 841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim \n",
    "\n",
    "from datasets import load_dataset, DatasetDict, load_metric\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, ASTConfig, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "AUDIO_DIR = \"./data/\"\n",
    "CSV_DIR = \"./data/metadata_compiled.csv\"\n",
    "FILE_TYPE = \".mp3\"\n",
    "\n",
    "# model\n",
    "SAMPLING_RATE = 16000\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "CHECKPOINT = 'MIT/ast-finetuned-audioset-10-10-0.4593'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.csv file loading\n",
    "df = pd.read_csv(CSV_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Explore the dataset through code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many samples does the dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check no. samples\n",
    "print(f'Number of samples : {df.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. How many classes? How many samples per class? Show a histogram of the number of intances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of classes: {len(df[\"status\"].unique())}.\\n\\\n",
    "    Classes: {df[\"status\"].unique()}\\n\\\n",
    "    {pd.value_counts(df[\"status\"], dropna=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df['status'], dropna=False).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Play a random sample from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing healthy\n",
    "healthy = df[df['status'] == 'healthy'].sample()['uuid'].item()\n",
    "path = AUDIO_DIR + healthy + FILE_TYPE\n",
    "print(path)\n",
    "y, sr = torchaudio.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing COVID-19\n",
    "covid = df[df['status'] == 'COVID-19'].sample()['uuid'].item()\n",
    "path = AUDIO_DIR + covid + FILE_TYPE\n",
    "y, sr = torchaudio.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing symptomatic\n",
    "symptomatic = df[df['status'] == 'symptomatic'].sample()['uuid'].item()\n",
    "path =  AUDIO_DIR + symptomatic + FILE_TYPE\n",
    "y, sr = torchaudio.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Describe if/how you think the data distribution will affect training of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Decide what part of the dataset to use; all, some classes, some samples. Motivate your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use a neural network of your own chose to classify the dataset. Explain your choice and at least one alternative. Document your experiences:."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Spectogram Transformer Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a Dataset class\n",
    "\n",
    "We create a custom Dataset class to load our cough files. \n",
    "\n",
    "In Pytorch the Dataset class has to override the functions: **\\__len__** and **\\__getitem__**, where **\\__len__** returns the amount of files in the dataset, and **\\__getitem__** returns the file and label for each file index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatatset(Dataset):\n",
    "    def __init__(self, audio_dir, class_csv):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.df = pd.read_csv(class_csv)\n",
    "\n",
    "        self.audio_dir_list = os.listdir(self.audio_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_dir_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #Loading the audio file\n",
    "        audio_file_path = os.path.join(self.audio_dir, self.audio_dir_list[idx])\n",
    "        waveform, sample_rate = ta.load(audio_file_path, normalize=True)\n",
    "        \n",
    "        #Transforming to mel spectogram\n",
    "        transform = ta.transforms.MelSpectrogram(sample_rate, n_mels=32)\n",
    "        mel_specgram = transform(waveform) \n",
    "\n",
    "        #Loading the label\n",
    "        audio_file_name = self.audio_dir_list[idx].replace(FILE_TYPE, '')\n",
    "\n",
    "        i =  self.df[ self.df['uuid']==audio_file_name].index.values\n",
    "        label =  self.df[\"status\"].loc[ self.df.index[i].values[0]]\n",
    "        #Convert the label from string to a number. Healthy = 0, Symptomaic = 1, Covid = 2\n",
    "        if label == 'healthy':\n",
    "            label = 0\n",
    "        elif label == 'symptomatic':\n",
    "            label == 1\n",
    "        else:\n",
    "            label == 2\n",
    "\n",
    "\n",
    "        return mel_specgram, label \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the Dataset class\n",
    "\n",
    "We can quickly create a an instance of the AudioDataset class and print out values for an item in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = AudioDatatset(AUDIO_DIR, CSV_DIR)\n",
    "print(test.__getitem__(0))\n",
    "print(test.__len__())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a DataModule class\n",
    "\n",
    "Pytorch also has a DataModule class that loads the data from the Dataset class we just made. In this class we split the dataset into training, validation and testing with a **70/20/10** split. This DataModule class also allows us to set the batch size, number of workers and more for the training. Since we are using Pytorch lightning, we need to have the following functions: **prepare_data, setup, train_dataloader, val_dataloader and test_dataloader.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModuleClass(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        #self.transform = transforms.MelSpectrogram(sample_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.audio_files =  []\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        pass\n",
    "        #Define steps that should be done\n",
    "        #only on one GPU, like getting data\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        #Apply melSpectogram transform\n",
    "        self.audio_files = AudioDatatset(AUDIO_DIR, CSV_DIR)\n",
    "\n",
    "        #Splitting manually \n",
    "        audio_len = self.audio_files.__len__()\n",
    "        train_size = round(audio_len * 0.7)\n",
    "        val_size = round(audio_len * 0.2)\n",
    "        test_size = audio_len - train_size - val_size\n",
    "        \n",
    "        self.train_data, self.val_data, self.test_data = random_split(self.audio_files, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, self.batch_size, num_workers=2, pin_memory=True, persistent_workers=True)       \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, self.batch_size, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, self.batch_size, num_workers=2, pin_memory=True, persistent_workers=True)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vini trying model stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, ASTConfig\n",
    "\n",
    "# must be the same -> model and tokenizer/feature extractor\n",
    "# right now with default values\n",
    "\n",
    "config = ASTConfig()\n",
    "\n",
    "# basically tokenizer\n",
    "# input normalization: mean = 0, std = 0.5\n",
    "\n",
    "#feature_extractor = ASTFeatureExtractor(config, sampling_rate=sr, num_mel_bins=32, mean=0, std=0.5)\n",
    "\n",
    "model = ASTForAudioClassification(config)\n",
    "\n",
    "train_loader = DataModuleClass(BATCH_SIZE)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1, accelerator='gpu', devices=1, log_every_n_steps=25)\n",
    "\n",
    "trainer.fit(model, train_loader)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load soundfiles into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must have metadata.csv with 'file_name' column to have also the features\n",
    "# unsplitted dataset\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=AUDIO_DIR, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting up the data : {training - 70%, validation - 20%, test - 10%}\n",
    "# shuffle=True\n",
    "train_testvalid = dataset.train_test_split(test_size=0.3)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=1/3)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train' : train_testvalid['train'],\n",
    "    'test' : test_valid['test'],\n",
    "    'valid' : test_valid['train']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the labels for classification\n",
    "labels = list(df[\"status\"].unique()[1:])\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams again \n",
    "# TODO : move up\n",
    "max_duration = 1\n",
    "\n",
    "hidden_dim = 768\n",
    "\n",
    "max_seq_length = max_duration * SAMPLING_RATE\n",
    "max_frames = 49\n",
    "max_epochs = 2\n",
    "\n",
    "is_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for [RANDOM WEIGHTS] - no pretraining\n",
    "config = ASTConfig(\n",
    "    hidden_size=768, # default : 768\n",
    "    num_hidden_layers=12, # default : 12\n",
    "    hidden_dropout_prob= 0.0 # def.: 0.0\n",
    "    attention_probs_dropout_prob=0.0 # def.: 0.0\n",
    ")\n",
    "\n",
    "# basically tokenizer\n",
    "# input normalization: mean = 0, std = 0.5\n",
    "feature_extractor = ASTFeatureExtractor(config, sampling_rate=SAMPLING_RATE, num_mel_bins=32, mean=0, std=0.5)\n",
    "\n",
    "model = ASTForAudioClassification(config)\n",
    "# weights must be the same for the model and the tokenizer/feature extractor "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pretrained session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor and model\n",
    "feature_extractor = ASTFeatureExtractor(\n",
    "    CHECKPOINT\n",
    ")\n",
    "\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    CHECKPOINT, \n",
    "    num_labels=len(labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map function\n",
    "def preprocess(examples):\n",
    "    audio_arr = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arr,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    label = [int(label2id[x]) for x in examples[\"label\"]]\n",
    "    \n",
    "    inputs[\"label\"] = label\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping datasets -> feature extraction\n",
    "train_ = dataset['train'].map(preprocess, remove_columns=[\"audio\"], batched=True)\n",
    "valid_ = dataset['valid'].map(preprocess, remove_columns=[\"audio\"], batched=True)\n",
    "test_ = dataset['test'].map(preprocess, remove_columns=[\"audio\"], batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the optimizer and the scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    metrics = dict()\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy_metric = load_metric('accuracy')\n",
    "    precision_metric = load_metric('precision')\n",
    "    recall_metric = load_metric('recall')\n",
    "    f1_metric = load_metric('f1')\n",
    "\n",
    "    metrics.update(accuracy_metric.compute(predictions=preds, references=labels))\n",
    "    metrics.update(precision_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
    "    metrics.update(recall_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
    "    metrics.update(f1_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"cough-project\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hyperparams for Trainer\n",
    "training_arg = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    #num_train_epochs= max_epochs,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    max_steps=1000,\n",
    "    logging_steps=50,\n",
    "    eval_steps=200, \n",
    "    eval_accumulation_steps=1, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=50,\n",
    "    save_total_limit=2,\n",
    "    #metric_for_best_model='accuracy'\n",
    "    )\n",
    "\n",
    "# defining trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arg,\n",
    "    train_dataset=train_,\n",
    "    eval_dataset=valid_,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers = (optimizer, scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training session\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "predictions = trainer.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model weigths into files\n",
    "model.save_pretrained('./saved_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Discuss at least four relevant hyper-parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate : \\\n",
    "Epoch Numer : \\\n",
    "Batch Size : \\\n",
    "Optimizer: \\\n",
    "Layer Number : [??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "# epoch number\n",
    "# mini-batch size\n",
    "\n",
    "# I don't know what is the 4th one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Experiment with the effect of different batch sizes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Experiment with the effect of different learning rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Experiment with different number of network layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Implement at least two data agumentation techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Discuss what influences the memory use of a solution such as yours. What can be done to reduce this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3f78a94c79aa2b4cde3db52c6d7f9e98cfa173f2ab20b939f1d7db0be852c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
