{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Miniproject - Audio\n",
    "\n",
    "AVS 8th Semester - Group 841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/deepl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio as ta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim \n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparaeters\n",
    "\n",
    "AUDIO_DIR = \"./data/\"\n",
    "CSV_DIR = \"./data/metadata_compiled.csv\"\n",
    "FILE_TYPE = \".mp3\"\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Explore the dataset through code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many samples does the dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data!\n",
    "df = pd.read_csv(CSV_DIR)\n",
    "\n",
    "#Check no. samples\n",
    "print(f'Number of samples : {df.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. How many classes? How many samples per class? Show a histogram of the number of intances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of classes: {len(df[\"status\"].unique())}.\\n\\\n",
    "    Classes: {df[\"status\"].unique()}\\n\\\n",
    "    {pd.value_counts(df[\"status\"], dropna=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df['status'], dropna=False).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Play a random sample from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing healthy\n",
    "healthy = df[df['status'] == 'healthy'].sample()['uuid'].item()\n",
    "path = AUDIO_DIR + healthy + FILE_TYPE\n",
    "y, sr = librosa.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing COVID-19\n",
    "covid = df[df['status'] == 'COVID-19'].sample()['uuid'].item()\n",
    "path = AUDIO_DIR + covid + FILE_TYPE\n",
    "y, sr = librosa.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing symptomatic\n",
    "symptomatic = df[df['status'] == 'symptomatic'].sample()['uuid'].item()\n",
    "path =  AUDIO_DIR + symptomatic + FILE_TYPE\n",
    "y, sr = librosa.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Describe if/how you think the data distribution will affect training of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Decide what part of the dataset to use; all, some classes, some samples. Motivate your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use a neural network of your own chose to classify the dataset. Explain your choice and at least one alternative. Document your experiences:."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Spectogram Transformer Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a Dataset class\n",
    "\n",
    "We create a custom Dataset class to load our cough files. \n",
    "\n",
    "In Pytorch the Dataset class has to override the functions: **\\__len__** and **\\__getitem__**, where **\\__len__** returns the amount of files in the dataset, and **\\__getitem__** returns the file and label for each file index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatatset(Dataset):\n",
    "    def __init__(self, audio_dir, class_csv):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.df = pd.read_csv(class_csv)\n",
    "\n",
    "        self.audio_dir_list = os.listdir(self.audio_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_dir_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #Loading the audio file\n",
    "        audio_file_path = os.path.join(self.audio_dir, self.audio_dir_list[idx])\n",
    "        waveform, sample_rate = ta.load(audio_file_path, normalize=True)\n",
    "        \n",
    "        #Transforming to mel spectogram\n",
    "        transform = ta.transforms.MelSpectrogram(sample_rate, n_mels=32)\n",
    "        mel_specgram = transform(waveform) \n",
    "\n",
    "        #Loading the label\n",
    "        audio_file_name = self.audio_dir_list[idx].replace(FILE_TYPE, '')\n",
    "\n",
    "        i =  self.df[ self.df['uuid']==audio_file_name].index.values\n",
    "        label =  self.df[\"status\"].loc[ self.df.index[i].values[0]]\n",
    "        #Convert the label from string to a number. Healthy = 0, Symptomaic = 1, Covid = 2\n",
    "        if label == 'healthy':\n",
    "            label = 0\n",
    "        elif label == 'symptomatic':\n",
    "            label == 1\n",
    "        else:\n",
    "            label == 2\n",
    "\n",
    "\n",
    "        return mel_specgram, label \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the Dataset class\n",
    "\n",
    "We can quickly create a an instance of the AudioDataset class and print out values for an item in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = AudioDatatset(AUDIO_DIR, CSV_DIR)\n",
    "print(test.__getitem__(0))\n",
    "print(test.__len__())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a DataModule class\n",
    "\n",
    "Pytorch also has a DataModule class that loads the data from the Dataset class we just made. In this class we split the dataset into training, validation and testing with a **70/20/10** split. This DataModule class also allows us to set the batch size, number of workers and more for the training. Since we are using Pytorch lightning, we need to have the following functions: **prepare_data, setup, train_dataloader, val_dataloader and test_dataloader.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModuleClass(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        #self.transform = transforms.MelSpectrogram(sample_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.audio_files =  []\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        pass\n",
    "        #Define steps that should be done\n",
    "        #only on one GPU, like getting data\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        #Apply melSpectogram transform\n",
    "        self.audio_files = AudioDatatset(AUDIO_DIR, CSV_DIR)\n",
    "\n",
    "        #Splitting manually \n",
    "        audio_len = self.audio_files.__len__()\n",
    "        train_size = round(audio_len * 0.7)\n",
    "        val_size = round(audio_len * 0.2)\n",
    "        test_size = audio_len - train_size - val_size\n",
    "        \n",
    "        self.train_data, self.val_data, self.test_data = random_split(self.audio_files, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, self.batch_size, num_workers=2, pin_memory=True, persistent_workers=True)       \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, self.batch_size, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, self.batch_size, num_workers=2, pin_memory=True, persistent_workers=True)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vini trying model stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, ASTConfig\n",
    "\n",
    "# must be the same -> model and tokenizer/feature extractor\n",
    "# right now with default values\n",
    "\n",
    "config = ASTConfig()\n",
    "\n",
    "# basically tokenizer\n",
    "# input normalization: mean = 0, std = 0.5\n",
    "\n",
    "#feature_extractor = ASTFeatureExtractor(config, sampling_rate=sr, num_mel_bins=32, mean=0, std=0.5)\n",
    "\n",
    "model = ASTForAudioClassification(config)\n",
    "\n",
    "train_loader = DataModuleClass(BATCH_SIZE)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1, accelerator='gpu', devices=1, log_every_n_steps=25)\n",
    "\n",
    "trainer.fit(model, train_loader)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading and pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# must have metadata.csv with 'file_name' column to have also the features\n",
    "# unsplitted dataset\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=AUDIO_DIR, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting up the data : {training - 70%, validation - 20%, test - 10%}\n",
    "# shuffle=True\n",
    "train_testvalid = dataset.train_test_split(test_size=0.3)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set = ['a', 'b', 'd']\n",
    "set.index('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train' : train_testvalid['train'],\n",
    "    'test' : test_valid['test'],\n",
    "    'valid' : test_valid['train']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the labels for classification\n",
    "labels = list(df[\"status\"].unique()[1:])\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i) # i <- str(i)\n",
    "    id2label[str(i)] = label # i <- str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams again \n",
    "# TODO : move up\n",
    "max_duration = 1\n",
    "\n",
    "samplin_rate = 16000\n",
    "batch_size = 8\n",
    "num_classes = len(labels)\n",
    "learning_rate = 1e-4\n",
    "hidden_dim = 768\n",
    "\n",
    "max_seq_length = max_duration * samplin_rate\n",
    "max_frames = 49\n",
    "max_epochs = 2\n",
    "\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "checkpoint = 'MIT/ast-finetuned-audioset-10-10-0.4593'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, ASTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for [RANDOM WEIGHTS] - no pretraining\n",
    "config = ASTConfig()\n",
    "\n",
    "# basically tokenizer\n",
    "# input normalization: mean = 0, std = 0.5\n",
    "feature_extractor = ASTFeatureExtractor(config, sampling_rate=sampling_rate, num_mel_bins=32, mean=0, std=0.5)\n",
    "\n",
    "model = ASTForAudioClassification(config)\n",
    "# weights must be the same for the model and the tokenizer/feature extractor "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pretrained session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ASTFeatureExtractor(\n",
    "    checkpoint,\n",
    "    #return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature extraction - using map()\n",
    "def preprocess(examples):\n",
    "    audio_arr = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arr,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    #label = []\n",
    "    #for x in examples[\"label\"]:\n",
    "        #label.append(int(label2id[x]))\n",
    "    \n",
    "    label = [int(label2id[x]) for x in examples[\"label\"]]\n",
    "    \n",
    "    inputs[\"label\"] = label\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = dataset['train'].map(\n",
    "    preprocess, remove_columns=[\"audio\"], batched=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "ds_valid = dataset['valid'].map(\n",
    "    preprocess, remove_columns=[\"audio\"], batched=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "ds_test = dataset['test'].map(\n",
    "    preprocess, remove_columns=[\"audio\"], batched=True, batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=len(labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run - yet\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, warmup_cosine(100, \n",
    "        max_lr=learning_rate,\n",
    "        total_steps=total_steps,\n",
    "        optimizer_lr=learning_rate,\n",
    "        min_lr=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1-score\": f1, \"recall-score\": recall, \"precision-score\": precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# the hyperparams for Trainer\n",
    "training_arg = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs= max_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model='loss',\n",
    "    learning_rate=learning_rate,\n",
    "    logging_steps=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arg,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataModuleClass(BATCH_SIZE)\n",
    "dm.setup()\n",
    "train_dataset = dm.train_dataloader()\n",
    "val_dataset = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "predictions = trainer.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model weigths into files\n",
    "model.save_pretrained('./saved_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Discuss at least four relevant hyper-parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate : \\\n",
    "Epoch Numer : \\\n",
    "Batch Size : \\\n",
    "Optimizer: \\\n",
    "Layer Number : [??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "# epoch number\n",
    "# mini-batch size\n",
    "\n",
    "# I don't know what is the 4th one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Experiment with the effect of different batch sizes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Experiment with the effect of different learning rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Experiment with different number of network layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Implement at least two data agumentation techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Discuss what influences the memory use of a solution such as yours. What can be done to reduce this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3f78a94c79aa2b4cde3db52c6d7f9e98cfa173f2ab20b939f1d7db0be852c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
