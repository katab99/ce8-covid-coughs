{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Miniproject - Audio\n",
    "\n",
    "AVS 8th Semester - Group 841"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dpl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, load_metric\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, ASTConfig, TrainingArguments, Trainer\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "AUDIO_DIR = \"./data/\"\n",
    "CSV_DIR = \"./data/metadata_compiled.csv\"\n",
    "FILE_TYPE = \".mp3\"\n",
    "\n",
    "# model\n",
    "SAMPLING_RATE = 16000\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-4\n",
    "CHECKPOINT = 'MIT/ast-finetuned-audioset-10-10-0.4593'\n",
    "\n",
    "MAX_DURATION = 1\n",
    "NUM_CLASSES = 3\n",
    "HIDDEN_LAYER_SIZE = 768\n",
    "NUM_HIDDEN_LAYERS = 12\n",
    "MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.csv file loading\n",
    "df = pd.read_csv(CSV_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Explore the dataset through code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many samples does the dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check no. samples\n",
    "print(f'Number of samples : {df.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. How many classes? How many samples per class? Show a histogram of the number of intances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of classes: {len(df[\"status\"].unique())}.\\n\\\n",
    "    Classes: {df[\"status\"].unique()}\\n\\\n",
    "    {pd.value_counts(df[\"status\"], dropna=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df['status'], dropna=False).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Play a random sample from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing healthy\n",
    "healthy = df[df['status'] == 'healthy'].sample()['uuid'].item()\n",
    "path = AUDIO_DIR + healthy + FILE_TYPE\n",
    "print(path)\n",
    "y, sr = torchaudio.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing COVID-19\n",
    "covid = df[df['status'] == 'COVID-19'].sample()['uuid'].item()\n",
    "path = AUDIO_DIR + covid + FILE_TYPE\n",
    "y, sr = torchaudio.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing symptomatic\n",
    "symptomatic = df[df['status'] == 'symptomatic'].sample()['uuid'].item()\n",
    "path =  AUDIO_DIR + symptomatic + FILE_TYPE\n",
    "y, sr = torchaudio.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Describe if/how you think the data distribution will affect training of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Decide what part of the dataset to use; all, some classes, some samples. Motivate your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use a neural network of your own chose to classify the dataset. Explain your choice and at least one alternative. Document your experiences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Spectogram Transformer Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load soundfiles into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 16226/16226 [00:00<00:00, 20058.77it/s] \n",
      "Found cached dataset audiofolder (/home/ubuntu/.cache/huggingface/datasets/audiofolder/default-ca7edb5d3275be7e/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    }
   ],
   "source": [
    "# must have metadata.csv with 'file_name' column to have also the features\n",
    "# unsplitted dataset\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=AUDIO_DIR, split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up the data : `training` - 70%, `validation` - 20%, `test` - 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = dataset.train_test_split(test_size=0.3)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=1/3)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train' : train_testvalid['train'],\n",
    "    'test' : test_valid['test'],\n",
    "    'valid' : test_valid['train']\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the labels from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the labels for classification\n",
    "labels = list(df[\"status\"].unique()[1:])\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model and Feature Extractor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell if you want to use `random weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for [RANDOM WEIGHTS] - no pretraining\n",
    "config = ASTConfig(\n",
    "    hidden_size=768, # default : 768\n",
    "    num_hidden_layers=12, # default : 12\n",
    "    hidden_dropout_prob= 0.0 # def.: 0.0\n",
    "    attention_probs_dropout_prob=0.0 # def.: 0.0\n",
    ")\n",
    "\n",
    "# basically tokenizer\n",
    "# input normalization: mean = 0, std = 0.5\n",
    "feature_extractor = ASTFeatureExtractor(config, sampling_rate=SAMPLING_RATE, num_mel_bins=32, mean=0, std=0.5)\n",
    "\n",
    "model = ASTForAudioClassification(config)\n",
    "# weights must be the same for the model and the tokenizer/feature extractor "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell if you want to use `pre-trained weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor and model\n",
    "feature_extractor = ASTFeatureExtractor(\n",
    "    CHECKPOINT\n",
    ")\n",
    "\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    CHECKPOINT, \n",
    "    num_labels=len(labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    audio_arr = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arr,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    label = [int(label2id[x]) for x in examples[\"label\"]]\n",
    "    \n",
    "    inputs[\"label\"] = label\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].map(preprocess, remove_columns=[\"audio\"], batched=True)\n",
    "dataset['valid'] = dataset['valid'].map(preprocess, remove_columns=[\"audio\"], batched=True)\n",
    "dataset['test']= dataset['test'].map(preprocess, remove_columns=[\"audio\"], batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `optimizer` and `scheduler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the optimizer and the scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric computing function : calculates `accuracy`, `precision`, `recall` and `f1 score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    metrics = dict()\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy_metric = load_metric('accuracy')\n",
    "    precision_metric = load_metric('precision')\n",
    "    recall_metric = load_metric('recall')\n",
    "    f1_metric = load_metric('f1')\n",
    "\n",
    "    metrics.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    metrics.update(precision_metric.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "    metrics.update(recall_metric.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "    metrics.update(f1_metric.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up `wandb` to visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"cough-project\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining `TrainingArguments` and `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hyperparams for Trainer\n",
    "training_arg = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    #num_train_epochs= max_epochs,S\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    max_steps=1000, \n",
    "    logging_steps=50,\n",
    "    eval_steps=200, \n",
    "    eval_accumulation_steps=1, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=50,\n",
    "    save_total_limit=2,\n",
    "    #metric_for_best_model='accuracy'\n",
    "    )\n",
    "\n",
    "# defining trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arg,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['valid'],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "predictions = trainer.predict(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model weigths into files\n",
    "model.save_pretrained('./saved_model/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Discuss at least four relevant hyper-parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate : \\\n",
    "Epoch Numer : \\\n",
    "Batch Size : \\\n",
    "Optimizer: \\\n",
    "Layer Number : [??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "# epoch number\n",
    "# mini-batch size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Experiment with the effect of different batch sizes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Experiment with the effect of different learning rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Experiment with different number of network layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Implement at least two data agumentation techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Discuss what influences the memory use of a solution such as yours. What can be done to reduce this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3f78a94c79aa2b4cde3db52c6d7f9e98cfa173f2ab20b939f1d7db0be852c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
