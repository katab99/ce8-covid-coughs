{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Miniproject - Audio\n",
    "\n",
    "AVS 8th Semester - Group 841"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/deeplearning/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, load_metric, concatenate_datasets\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, ASTConfig, TrainingArguments, Trainer\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "AUDIO_DIR = \"./data/\"\n",
    "CSV_DIR = \"./data/metadata_compiled.csv\"\n",
    "FILE_TYPE = \".mp3\"\n",
    "\n",
    "# model\n",
    "SAMPLING_RATE = 16000\n",
    "BATCH_SIZE = 4 # 4\n",
    "LEARNING_RATE = 1e-3\n",
    "CHECKPOINT = 'MIT/ast-finetuned-audioset-10-10-0.4593'\n",
    "\n",
    "MAX_DURATION = 1\n",
    "NUM_CLASSES = 3\n",
    "HIDDEN_LAYER_SIZE = 384 # 768 \n",
    "NUM_HIDDEN_LAYERS = 12\n",
    "HIDDEN_DROPOUT_PROB = 0.1\n",
    "ATTENTION_DROPOUT_PROB = 0.1\n",
    "\n",
    "MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.csv file loading\n",
    "df = pd.read_csv(CSV_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Explore the dataset through code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many samples does the dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples : 27550\n"
     ]
    }
   ],
   "source": [
    "#Check no. samples\n",
    "print(f'Number of samples : {df.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. How many classes? How many samples per class? Show a histogram of the number of intances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of classes: {len(df[\"status\"].unique())}.\\n\\\n",
    "    Classes: {df[\"status\"].unique()}\\n\\\n",
    "    {pd.value_counts(df[\"status\"], dropna=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df['status'], dropna=False).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Play a random sample from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing healthy\n",
    "healthy = df[df['status'] == 'healthy'].sample()['uuid'].item()\n",
    "path = AUDIO_DIR + healthy + FILE_TYPE\n",
    "print(path)\n",
    "y, sr = torchaudio.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing COVID-19\n",
    "covid = df[df['status'] == 'COVID-19'].sample()['uuid'].item()\n",
    "path = AUDIO_DIR + covid + FILE_TYPE\n",
    "y, sr = torchaudio.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing symptomatic\n",
    "symptomatic = df[df['status'] == 'symptomatic'].sample()['uuid'].item()\n",
    "path =  AUDIO_DIR + symptomatic + FILE_TYPE\n",
    "y, sr = torchaudio.load(path)\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Describe if/how you think the data distribution will affect training of a classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `healthy` data is over represented among the ohter classes, the model after training can összetéveszteni confuse `COVID-19` and `symptomatic`  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Decide what part of the dataset to use; all, some classes, some samples. Motivate your choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not use the data without labels as we cannot check if those classification would be correct."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use a neural network of your own chose to classify the dataset. Explain your choice and at least one alternative. Document your experiences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to train the dataset with `Audio Spectogram Transformer` model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load soundfiles into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 16226/16226 [00:01<00:00, 10007.34it/s]\n",
      "Found cached dataset audiofolder (/home/ubuntu/.cache/huggingface/datasets/audiofolder/default-5813fa48534e5405/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n",
      "Resolving data files: 100%|██████████| 6056/6056 [00:00<00:00, 11702.57it/s]\n",
      "Found cached dataset audiofolder (/home/ubuntu/.cache/huggingface/datasets/audiofolder/default-e1b877236e834087/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n",
      "                                                                         \r"
     ]
    }
   ],
   "source": [
    "# must have metadata.csv with 'file_name' column to have also the features\n",
    "# unsplitted dataset\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=AUDIO_DIR, split=\"train\")\n",
    "dataset_augmentation=load_dataset(\"audiofolder\", data_dir=\"./data_aug/\", split=\"train\")\n",
    "full_dataset= concatenate_datasets([dataset,dataset_augmentation])\n",
    "ds = full_dataset.class_encode_column(\"label\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up the data : `training` - 70%, `validation` - 20%, `test` - 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = ds.train_test_split(test_size=1/3,stratify_by_column='label') #Split dataset into train 70% and test 30%\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=1/3)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train' : train_testvalid['train'],\n",
    "    'test' : test_valid['test'],\n",
    "    'valid' : test_valid['train']\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the labels from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the labels for classification\n",
    "labels = list(df.sort_values('status')[\"status\"].unique()[:-1])\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model and Feature Extractor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell if you want to use `random weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ASTConfig(\n",
    "    hidden_size=768, # default : 768\n",
    "    #num_hidden_layers=12, # default : 12\n",
    "    hidden_dropout_prob= 0.0, # def.: 0.0\n",
    "    attention_probs_dropout_prob=0.0 # def.: 0.0\n",
    ")\n",
    "\n",
    "# input normalization: mean = 0, std = 0.5\n",
    "feature_extractor = ASTFeatureExtractor(config, sampling_rate=SAMPLING_RATE, num_mel_bins=32, mean=0, std=0.5)\n",
    "\n",
    "model = ASTForAudioClassification(config)\n",
    "# weights must be the same for the model and the tokenizer/feature extractor "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell if you want to use `pre-trained weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- audio_spectrogram_transformer.embeddings.cls_token: found shape torch.Size([1, 1, 768]) in the checkpoint and torch.Size([1, 1, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.embeddings.distillation_token: found shape torch.Size([1, 1, 768]) in the checkpoint and torch.Size([1, 1, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.embeddings.position_embeddings: found shape torch.Size([1, 1214, 768]) in the checkpoint and torch.Size([1, 1214, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 1, 16, 16]) in the checkpoint and torch.Size([384, 1, 16, 16]) in the model instantiated\n",
      "- audio_spectrogram_transformer.embeddings.patch_embeddings.projection.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.0.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.1.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.2.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.3.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.4.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.5.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.6.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.7.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.8.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.9.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.10.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.attention.attention.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.attention.attention.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.attention.attention.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.attention.attention.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.attention.attention.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.attention.attention.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([384, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([3072, 384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([384, 3072]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.layernorm_before.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.layernorm_before.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.layernorm_after.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.encoder.layer.11.layernorm_after.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.layernorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- audio_spectrogram_transformer.layernorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- classifier.layernorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- classifier.layernorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([384]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([3, 384]) in the model instantiated\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = ASTConfig(\n",
    "    hidden_size=HIDDEN_LAYER_SIZE, # default : 768\n",
    "    #num_hidden_layers=NUM_HIDDEN_LAYERS, # default : 12\n",
    "    hidden_dropout_prob=HIDDEN_DROPOUT_PROB, # def.: 0.0\n",
    "    attention_probs_dropout_prob=ATTENTION_DROPOUT_PROB, # def.: 0.0\n",
    "    num_labels=len(labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "feature_extractor = ASTFeatureExtractor(\n",
    "    CHECKPOINT\n",
    ")\n",
    "\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    CHECKPOINT, \n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    audio_arr = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arr,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    label = [x for x in examples[\"label\"]]\n",
    "    \n",
    "    inputs[\"label\"] = label\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].map(preprocess, remove_columns=[\"audio\"], batched=True)\n",
    "dataset['valid'] = dataset['valid'].map(preprocess, remove_columns=[\"audio\"], batched=True)\n",
    "dataset['test']= dataset['test'].map(preprocess, remove_columns=[\"audio\"], batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `optimizer` and `scheduler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=30, gamma=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric computing function : calculates `accuracy`, `precision`, `recall` and `f1 score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    metrics = dict()\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy_metric = load_metric('accuracy')\n",
    "    precision_metric = load_metric('precision')\n",
    "    recall_metric = load_metric('recall')\n",
    "    f1_metric = load_metric('f1')\n",
    "\n",
    "    metrics.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    metrics.update(precision_metric.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "    metrics.update(recall_metric.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "    metrics.update(f1_metric.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up `wandb` to visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"cough-project\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining `TrainingArguments` and `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hyperparams for Trainer\n",
    "training_arg = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    #num_train_epochs= MAX_APOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    max_steps=1000, \n",
    "    logging_steps=50,\n",
    "    eval_steps=200, \n",
    "    eval_accumulation_steps=1, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=50,\n",
    "    save_total_limit=2,\n",
    "    #metric_for_best_model='accuracy'\n",
    "    )\n",
    "\n",
    "# defining trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arg,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['valid'],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvmatth\u001b[0m (\u001b[33mdeepl-coughs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/ce8-covid-coughs/wandb/run-20230329_155757-psukmv09</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deepl-coughs/cough-project/runs/psukmv09' target=\"_blank\">dark-river-39</a></strong> to <a href='https://wandb.ai/deepl-coughs/cough-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deepl-coughs/cough-project' target=\"_blank\">https://wandb.ai/deepl-coughs/cough-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deepl-coughs/cough-project/runs/psukmv09' target=\"_blank\">https://wandb.ai/deepl-coughs/cough-project/runs/psukmv09</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/1000 00:00 < 10:52, 1.53 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "predictions = trainer.predict(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model weigths into files\n",
    "model.save_pretrained('./saved_model/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Discuss at least four relevant hyper-parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate : \\\n",
    "Epoch Numer : \\\n",
    "Batch Size : \\\n",
    "Optimizer: \\\n",
    "Layer Number : [??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "# epoch number\n",
    "# mini-batch size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Experiment with the effect of different batch sizes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Experiment with the effect of different learning rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Experiment with different number of network layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Implement at least two data agumentation techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Discuss what influences the memory use of a solution such as yours. What can be done to reduce this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3f78a94c79aa2b4cde3db52c6d7f9e98cfa173f2ab20b939f1d7db0be852c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
